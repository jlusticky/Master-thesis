%=========================================================================
% (c) 2014, 2015 Josef Lusticky

\chapter{Linux Kernel}
Linux version 3.10 consists of nearly 17 million lines of code~\cite{linux-by-numbers}.



\section{Packet Processing Techniques}
The traditional way of processing packets from NIC is interrupt-driven~\cite{linux-kernel-networking}.
Each incoming packet is an asynchronous event which causes interrupt.
%Similarly every complete transmission of an outgoing packet causes interrupt.
Such a method of packet processing stopped to be sufficient with the emerge of 100~Mbps cards. % TODO cite

---

The poll method introduced in the New API (NAPI) is designed
to provide performance improvement of high-speed networking.
NAPI was first introduced in the 2.5/2.6 kernel and was backported to the 2.4.20 kernel~\cite{linux-kernel-networking}.
With NAPI, under high load, the network
device driver works in polling mode and not in interrupt-driven mode~\cite{linux-kernel-networking}.
This means that each received packet does not trigger an interrupt.
Instead the packets are buffered in the driver, and the kernel polls the driver from time to time to
fetch the packets.

This reduces interrupt load on the system and can lower CPU utilization under heavy load,
but will increase latency as packets are not processed as quickly~\cite{linux-foundation-napi}.

Almost all of the drivers support this feature~\cite{linux-kernel-networking}.

When under a heavy load, the packets are lost because of not enough space in NIC buffer.
The packets to be lost are not fed into the network stack, so they take no CPU time~\cite{haifux-lecture}.

Using NAPI improves performance under high load. For sockets applications that need the lowest
possible latency and are willing to pay a cost of higher CPU utilization, Linux has added a capability for Busy Polling on
Sockets from kernel 3.11 and later~\cite{linux-kernel-networking}.



---

The kernel already supports TCP segmentation offloading (TSO),
where an adapter can create TCP packets out of a large array of data.
TSO reduces the necessary CPU power, bus overhead, and cache impact to send a series of packets,
but it still does not require that the adapter actually know
anything about specific TCP connections~\cite{linux-and-tcp-offload-engines}.
TCP Segment Offload - kernel is not dealing with segmenting.
ethtool -K eth1 tso on
ethtool -k eth1

TSO is well supported in Linux.
For systems which are engaged mainly in the sending of data,
it's sufficient to make 10GB work at full speed~\cite{jls2009-gro}.


While TSO and other simple tasks implemented in hardware help reducing the CPU load,
the kernel still has to deal the TCP states and ACKs.

To mitigate CPU load spent on TCP overhead completely, the TCP Offload Engine (TOE) is implemented in several NICs.
TOE features a full TCP/IP implementation in adapter.
However, Linux has never supported the TOE features of any network cards~\cite{linux-and-tcp-offload-engines}.

Vendors have made modifications to Linux to support TOE, and these changes have been submitted changes for kernel inclusion but were rejected.
The reasons why Linux engineers currently feel that full network stack offload (TCP Offload Engine, TOE) has little merit.
http://www.linuxfoundation.org/collaborate/workgroups/networking/toe

TOE shorts out much of the Linux networking code.
In the process, it cuts out features like netfilter, traffic control, and more.
The Linux networking stack is easy to fix when a bug or security issue comes up. If a security problem turns up in a TOE adapter, instead, there is very little which can be done to fix it.
A long-term maintaince burden~\cite{linux-and-tcp-offload-engines}.

Jeff Garzik claims that 100Mb/s TOE adaptors
(which used to be the bleeding-edge high speed)
are now slower than the Linux networking stack.
So any performance advantage from TOE is a temporary thing, but, once it is merged, the code must be supported forever.

David Miller: There are ways to obtain TOE's performance without
necessitating stateful support in the cards, everything that's
worthwhile can be done with stateless offloads.

---
NAPI is a proven (www.cyberus.ca/~hadi/usenix-paper.tgz) technique
to improve network performance on Linux.
http://lwn.net/2002/0321/a/napi-howto.php3
---

NAPI ("New API", though it is not so new anymore, since linux kernel 2.6) is an extension to the device driver packet processing framework, which is designed to improve the performance of high-speed networking~\cite{linux-foundation-napi}.

NAPI works through:

Interrupt mitigation (coalescing)
 Improves throughput but degrades latency.
    High-speed networking can create thousands of interrupts per second, all of which tell the system something it already knew: it has lots of packets to process. NAPI allows drivers to run with (some) interrupts disabled during times of high traffic, with a corresponding decrease in system load.

Packet throttling 
    When the system is overwhelmed and must drop packets, it's better if those packets are disposed of before much effort goes into processing them. NAPI-compliant drivers can often cause packets to be dropped in the network adaptor itself, before the kernel sees them at all.

More careful packet treatment, with special care taken to avoid reordering packets. Out-of-order packets can be a significant performance bottleneck. 


-----


NAPI is an interrupt mitigation mechanism used with network devices. When network traffic is heavy,
the kernel can safely predict that incoming packets will be available anytime it gets around to looking,
so there is no need to have the adapter interrupting it (possibly thousands of times per second)
to tell it about those packets. So a NAPI-compliant driver will turn off the packet receive interrupt
and provide a poll() method to the kernel.
When the kernel is ready to deal with more packets, poll() will be called with a maximum number
of packets it is allowed to feed into the kernel; it should process up to that many packets and quit.
%http://lwn.net/Articles/214457/

-----

Network interfaces, like most reasonable peripheral devices,
are capable of interrupting the CPU whenever a packet arrives.
But even a moderately busy interface can handle hundreds or thousands of packets per second;
per-packet interrupts would quickly overwhelm the processor with interrupt-handling work,
leaving little time for getting useful tasks done. So most interface drivers will disable the per-packet
interrupt when the traffic level is high enough and, with cooperation from the core networking stack,
occasionally poll the device for new packets. There are a number of advantages to doing things this way:
vast numbers of interrupts can be avoided, incoming packets can be more efficiently processed in batches,
and, if packets must be dropped in response to load, they can be discarded in
the interface before they ever hit the network stack. Polling is thus a win for
almost all situations where there is any significant amount of traffic at all~\cite{low-latency-ethernet-device-polling}.

---

NAPI is pretty good, but optimized for throughput

Latency is high by default (especially for Ethernet)
Jitter is unpredictable by default

netperf

Low Latency Sockets:
%http://www.linuxplumbersconf.org/2012/wp-content/uploads/2012/09/2012-lpc-Low-Latency-Sockets-slides-brandeburg.pdf




\section{Routing subsystem}
Forwarding information base.
Linux kernel supports multiple routing tables and
there are two routing tables by default (in case of non policy routing)~\cite{haifux-lecture}.
local FIB table
main FIB table

include/net/ip\_fib.h

Regardless of how many there are routing tables, there is always one routing cache ("route -C").


Routes can be introduced to the main routing table in one of three ways:
by system administrator command ('ip route add' or 'route add' which is obsolete)
by routing daemons
as a result of ICMP Redirect

A routing table is implemented by struct fib\_table.
There is one routing cache, regardless of how many routing tables there are.
The routing cache can be viewed by running 'ip route show cache' or 'route -C' (/proc/net/rt\_cache).

The routing cache consists of rtable elements.


---

Linux routing can be manipulated using netlink interface.
This interface provides easier use than ioctl(2).
This interface provides traditional socket-based messages.
socket(AF\_NETLINK, ...);
send(..);

Rtnetlink allows the kernel's routing tables to be read and altered.

netlink(3) - system call - netlink family NETLINK\_ROUTE
libnetlink , libnl
