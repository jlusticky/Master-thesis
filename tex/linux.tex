%=========================================================================
% (c) 2014, 2015 Josef Lusticky

\chapter{Linux Kernel Packet Processing}

Linux version 3.10 consists of nearly 17 million lines of code.
http://www.cnet.com/news/linux-development-by-the-numbers-big-and-getting-bigger/

Processing packets in the Linux kernel.
There are two modes of processing packets from NIC.
The traditional way is interrupt-driven - each incomming packet is an
asynchronous event which causes interrupt.

The poll method introduced in the New API (NAPI) - only the first incomming packet causes interrupt
and other packets are polled.
NAPI is designed to improve the performance of high-speed networking.
The driver using NAPI provides the poll function to the kernel for reading received packets.

Most of the new drivers support this feature.
When under a heavy load, the packets are lost because of not enough space in NIC buffer.
The packets to be lost are not fed into the network stack, so they take no CPU time.
http://www.haifux.org/lectures/172/netLec.pdf


The kernel already supports TCP segmentation offloading (TSO), where an adapter can create TCP packets out of a large array of data.
TSO reduces the necessary CPU power, bus overhead, and cache impact to send a series of packets,
but it still does not require that the adapter actually know anything about specific TCP connections.
http://lwn.net/Articles/148697/
TCP Segment Offload - kernel is not dealing with segmenting.
ethtool -K eth1 tso on
ethtool -k eth1

TSO is well supported in Linux; for systems which are engaged mainly in the sending of data, it's sufficient to make 10GB work at full speed.
https://lwn.net/Articles/358910/


While TSO and other simple tasks implemented in hardware help reducing the CPU load,
the kernel still has to deal the TCP states and ACKs.

To mitigate CPU load spent on TCP overhead completely, the TCP Offload Engine (TOE) is implemented in several NICs.
TOE features a full TCP/IP implementation in adapter.
However, Linux has never supported the TOE features of any network cards~\cite{http://lwn.net/Articles/148697/}.

Vendors have made modifications to Linux to support TOE, and these changes have been submitted changes for kernel inclusion but were rejected.
The reasons why Linux engineers currently feel that full network stack offload (TCP Offload Engine, TOE) has little merit.
http://www.linuxfoundation.org/collaborate/workgroups/networking/toe

TOE shorts out much of the Linux networking code.
In the process, it cuts out features like netfilter, traffic control, and more.
The Linux networking stack is easy to fix when a bug or security issue comes up. If a security problem turns up in a TOE adapter, instead, there is very little which can be done to fix it.
A long-term maintaince burden. http://lwn.net/Articles/148697/

Jeff Garzik claims that 100Mb/s TOE adaptors
(which used to be the bleeding-edge high speed)
are now slower than the Linux networking stack.
So any performance advantage from TOE is a temporary thing, but, once it is merged, the code must be supported forever.

David Miller: There are ways to obtain TOE's performance without
necessitating stateful support in the cards, everything that's
worthwhile can be done with stateless offloads.

---


\section{NAPI}

NAPI is a proven (www.cyberus.ca/~hadi/usenix-paper.tgz) technique
to improve network performance on Linux.
http://lwn.net/2002/0321/a/napi-howto.php3
---

NAPI ("New API", though it is not so new anymore, since linux kernel 2.6) is an extension to the device driver packet processing framework, which is designed to improve the performance of high-speed networking~\cite{linux-foundation-napi}.

NAPI works through:

Interrupt mitigation (coalescing)
 Improves throughput but degrades latency.
    High-speed networking can create thousands of interrupts per second, all of which tell the system something it already knew: it has lots of packets to process. NAPI allows drivers to run with (some) interrupts disabled during times of high traffic, with a corresponding decrease in system load.

Packet throttling 
    When the system is overwhelmed and must drop packets, it's better if those packets are disposed of before much effort goes into processing them. NAPI-compliant drivers can often cause packets to be dropped in the network adaptor itself, before the kernel sees them at all.

More careful packet treatment, with special care taken to avoid reordering packets. Out-of-order packets can be a significant performance bottleneck. 


NAPI additions to the kernel do not break backward compatibility and drivers may still process completions directly in interrupt context if necessary.

http://lwn.net/Articles/30107/

-----


NAPI is an interrupt mitigation mechanism used with network devices. When network traffic is heavy, the kernel can safely predict that incoming packets will be available anytime it gets around to looking, so there is no need to have the adapter interrupting it (possibly thousands of times per second) to tell it about those packets. So a NAPI-compliant driver will turn off the packet receive interrupt and provide a poll() method to the kernel. When the kernel is ready to deal with more packets, poll() will be called with a maximum number of packets it is allowed to feed into the kernel; it should process up to that many packets and quit.
http://lwn.net/Articles/214457/

-----

Network interfaces, like most reasonable peripheral devices, are capable of interrupting the CPU whenever a packet arrives. But even a moderately busy interface can handle hundreds or thousands of packets per second; per-packet interrupts would quickly overwhelm the processor with interrupt-handling work, leaving little time for getting useful tasks done. So most interface drivers will disable the per-packet interrupt when the traffic level is high enough and, with cooperation from the core networking stack, occasionally poll the device for new packets. There are a number of advantages to doing things this way: vast numbers of interrupts can be avoided, incoming packets can be more efficiently processed in batches, and, if packets must be dropped in response to load, they can be discarded in the interface before they ever hit the network stack. Polling is thus a win for almost all situations where there is any significant amount of traffic at all. 
http://lwn.net/Articles/551284/

---

NAPI is pretty good, but optimized for throughput

Latency is high by default (especially for Ethernet)
Jitter is unpredictable by default

netperf

http://www.linuxplumbersconf.org/2012/wp-content/uploads/2012/09/2012-lpc-Low-Latency-Sockets-slides-brandeburg.pdf
