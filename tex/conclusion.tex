%=========================================================================
% (c) 2014, 2015 Josef Lusticky

\chapter{Conclusion}\label{chap:conclusion}
The thesis provides description of the principles behind packet processing in the Linux kernel, as well as
a general description of the 40 Gigabit Ethernet protocol and its performance limitation.
An advanced knowledge of the principles described in the thesis is required to
perform the correct system settings for maximum routing performance with the GNU/Linux operating system.

The CentOS~7 operating system was installed to perform the measurements.
The system features Linux kernel based on version 3.10.
Additionally, upstream kernel 3.19.2 was installed.
The Linux kernel routing performance was measured under different scenarios, including
simulation of a real internet traffic, based on the data provided by the Amsterdam Internet Exchange.
Custom frame-size distribution was configured for this purpose and used in the experiments.
%TODO BGP
The overall routing performance of the Linux kernel is sufficient for routing 35.91~Gbps
of the simulated Internet traffic on Intel Xeon CPU.
The CPU features 8~physical cores and the HyperThreading technology (16~ logical cores).
The Mellanox ConnectX-3 EN adapter is able to scale the packet processing to 8~cores.%TODO
The Linux kernel is able to route 5.9~millions frames per second on 8~physical cores.

40Gbit software router with GNU/Linux may provide a reasonable alternative to proprietary hardware-based routers.
The main bottleneck of the default installation is the {\it{irqbalance}} daemon, which
does not set the optimal IRQ affinity for multiqueue network adapters.
Thus, the scaling mechanisms implemented in the Linux kernel are not used optimally.
The thesis describes how to set IRQ affinity manually for the purpose of maximum throughput.

The Linux kernel uses advantage of its scaling mechanisms to perform well in network processing,
however, a single-core packet processing must be improved to achieve better results.
The Linux kernel is able to route approx.~1~000~000 %TODO
frames per second utilising a single core.
This is also the limitation of a single network flow processing,
since the scaling mechanisms implemented in the Linux kernel and network adapters
are based on processing each flow by a different CPU.

The thesis can be further extended through a comparison against hardware-based routers.
The future work may compare latency, throughput or power consumption of both systems.
Additionally, various user-space frameworks focused on improving the GNU/Linux network performance may be evaluated,
such as Data Plane Development Kit.
Further work extending this thesis may include comparison of
the frameworks with the results presented in the thesis.
